# -*- coding: utf-8 -*-
"""Evaluation_Comparison.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yW62Ek5BBi9YZ3fvm_e5UYZ-QpxGUiL1
"""

PART 3: FINAL EVALUATION AND VERDICT (CLEAN GPU RUN)
--- Initial memory check ---

--- Loading model instance on device: CUDA ---
âœ… Model loaded successfully.
Unzipping development set...
Processingâ€‡devâ€‡files:â€‡100%â€‡999/999â€‡[00:00<00:00,â€‡3794.75it/s]Device set to use cuda:0
Final evaluation 100 examples par kiya ja raha hai.

Predictions generate ki ja rahi hain (Batch Size 1, Console Keep-Alive Active)...
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset

âš ï¸ Generation failed for example 13 due to: CUDA out of memory. Tried to allocate 3.60 GiB. GPU 0 has a total capacity of 22.16 GiB of which 2.39 GiB is free. Process 18315 has 19.77 GiB memory in use. Of the allocated memory 19.31 GiB is allocated by PyTorch, and 248.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Appending empty summary.
Your max_length is set to 20, but your input_length is only 5. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=2)
Generation complete! Processed 100 examples.

--- Calculating Metrics ---
/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning:
The secret `HF_TOKEN` does not exist in your Colab secrets.
To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.
You will be able to reuse this secret in all of your notebooks.
Please note that authentication is recommended but still optional to access public models or datasets.
  warnings.warn(
Downloadingâ€‡builderâ€‡script:â€‡â€‡6.27k/?â€‡[00:00<00:00,â€‡645kB/s]Downloadingâ€‡builderâ€‡script:â€‡â€‡7.95k/?â€‡[00:00<00:00,â€‡871kB/s]tokenizer_config.json:â€‡100%â€‡48.0/48.0â€‡[00:00<00:00,â€‡6.06kB/s]config.json:â€‡100%â€‡570/570â€‡[00:00<00:00,â€‡71.7kB/s]vocab.txt:â€‡100%â€‡232k/232kâ€‡[00:00<00:00,â€‡8.79MB/s]tokenizer.json:â€‡100%â€‡466k/466kâ€‡[00:00<00:00,â€‡6.50MB/s]model.safetensors:â€‡100%â€‡440M/440Mâ€‡[00:01<00:00,â€‡523MB/s]

==================================================
     ðŸ”¬ EXPERIMENT RESULTS & VERDICT ðŸ”¬
==================================================

--- BASELINE MODEL (Original Data) ---
  ROUGE-L Score: 12.41
  BERTScore (F1): 80.48

--- NEW MODEL (Cleaned Data, Stable Training) ---
  ROUGE-L Score: 8.81
  BERTScore (F1): 56.74

--------------------
      DECISION
--------------------
âš ï¸ KOI SUDHAAR NAHI. Cleaned data par model behtar perform nahi kar paya.

==================================================